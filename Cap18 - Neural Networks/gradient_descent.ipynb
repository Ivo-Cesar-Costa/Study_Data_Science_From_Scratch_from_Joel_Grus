{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd3f535-ca68-4e3a-9463-51cc19603e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from linear_algebra.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627802e5-0d56-40f6-851e-13ae1aaf10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456910db-11ac-49d2-bcbd-fd88cbdcd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6e07ab-5f11-4e9c-a4ea-9d65b766fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df514eb2-3523-4127-ae2b-44848df2afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f26470e-a7ef-47d1-86a8-d5b23e552f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1548e8-51e4-46c7-9019-0c9bca48e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80e76d5-12a2-492e-8577-d5da95049c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from linear_algebra import distance, add, scalar_multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f27744-e0c5-4478-a4f1-a6a9513afa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b900dbeb-ebe2-4252-8e24-9ed5832022e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80fe2af8-ee14-4777-b22f-a1d33d6ddd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64933bdc-df0b-4de9-a160-d808bf03a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error\n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0af754-ecdf-4f3d-a5bb-854590ab53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a375589b-c0ca-4cc9-a4b9-50d1c2190566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5f7894-6ee6-469d-890f-aafa571ad1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    xs = range(-10, 11)\n",
    "    actuals = [derivative(x) for x in xs]\n",
    "    estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "    \n",
    "    # plot to show they're basically the same\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "    plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
    "    plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
    "    plt.legend(loc=9)\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                    v: Vector,\n",
    "                                    i: int,\n",
    "                                    h: float) -> float:\n",
    "        \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "        w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
    "             for j, v_j in enumerate(v)]\n",
    "    \n",
    "        return (f(w) - f(v)) / h\n",
    "    \n",
    "    # \"Using the Gradient\" example\n",
    "    \n",
    "    # pick a random starting point\n",
    "    v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "        v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "        print(epoch, v)\n",
    "    \n",
    "    assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
    "    \n",
    "    # First \"Using Gradient Descent to Fit Models\" example\n",
    "    \n",
    "    from linear_algebra import vector_mean\n",
    "       \n",
    "    # Start with random values for slope and intercept.\n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    for epoch in range(5000):\n",
    "        # Compute the mean of the gradients\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "        # Take a step in that direction\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "    \n",
    "    # Minibatch gradient descent example\n",
    "    \n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        for batch in minibatches(inputs, batch_size=20):\n",
    "            grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "            theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "    \n",
    "    # Stochastic gradient descent example\n",
    "    \n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for x, y in inputs:\n",
    "            grad = linear_gradient(x, y, theta)\n",
    "            theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "\n",
    "#if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
