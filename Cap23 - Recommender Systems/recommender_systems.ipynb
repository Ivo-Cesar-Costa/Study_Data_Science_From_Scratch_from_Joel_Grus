{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d777312-54a1-4470-9c11-60b1bc3e31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8add770-c80d-4737-8174-14779678e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e0bbe6-f019-4ab2-aacc-2ab44e079e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Hadoop': 2,\n",
       "         'Big Data': 3,\n",
       "         'HBase': 3,\n",
       "         'Java': 3,\n",
       "         'Spark': 1,\n",
       "         'Storm': 1,\n",
       "         'Cassandra': 2,\n",
       "         'NoSQL': 1,\n",
       "         'MongoDB': 2,\n",
       "         'Postgres': 2,\n",
       "         'Python': 4,\n",
       "         'scikit-learn': 2,\n",
       "         'scipy': 1,\n",
       "         'numpy': 1,\n",
       "         'statsmodels': 2,\n",
       "         'pandas': 2,\n",
       "         'R': 4,\n",
       "         'statistics': 3,\n",
       "         'regression': 3,\n",
       "         'probability': 3,\n",
       "         'machine learning': 2,\n",
       "         'decision trees': 1,\n",
       "         'libsvm': 2,\n",
       "         'C++': 2,\n",
       "         'Haskell': 1,\n",
       "         'programming languages': 1,\n",
       "         'mathematics': 1,\n",
       "         'theory': 1,\n",
       "         'Mahout': 1,\n",
       "         'neural networks': 2,\n",
       "         'deep learning': 2,\n",
       "         'artificial intelligence': 2,\n",
       "         'MapReduce': 1,\n",
       "         'databases': 1,\n",
       "         'MySQL': 1,\n",
       "         'support vector machines': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_interests = Counter(interest\n",
    "                            for user_interests in users_interests\n",
    "                            for interest in user_interests)\n",
    "popular_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9342572-2d45-4797-8b7e-0f58c22b7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84030e77-e8ac-4cba-8b70-39dde9a28591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_popular_new_interests(\n",
    "        user_interests: List[str],\n",
    "        max_results: int = 5) -> List[Tuple[str, int]]:\n",
    "    suggestions = [(interest, frequency)\n",
    "                   for interest, frequency in popular_interests.most_common()\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6089fd96-e6a0-4321-b93b-3681b9c80d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'C++',\n",
       " 'Cassandra',\n",
       " 'HBase',\n",
       " 'Hadoop',\n",
       " 'Haskell',\n",
       " 'Java',\n",
       " 'Mahout',\n",
       " 'MapReduce',\n",
       " 'MongoDB',\n",
       " 'MySQL',\n",
       " 'NoSQL',\n",
       " 'Postgres',\n",
       " 'Python',\n",
       " 'R',\n",
       " 'Spark',\n",
       " 'Storm',\n",
       " 'artificial intelligence',\n",
       " 'databases',\n",
       " 'decision trees',\n",
       " 'deep learning',\n",
       " 'libsvm',\n",
       " 'machine learning',\n",
       " 'mathematics',\n",
       " 'neural networks',\n",
       " 'numpy',\n",
       " 'pandas',\n",
       " 'probability',\n",
       " 'programming languages',\n",
       " 'regression',\n",
       " 'scikit-learn',\n",
       " 'scipy',\n",
       " 'statistics',\n",
       " 'statsmodels',\n",
       " 'support vector machines',\n",
       " 'theory']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_interests = sorted({interest\n",
    "                           for user_interests in users_interests\n",
    "                           for interest in user_interests})\n",
    "unique_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c72c8-13d2-4189-83c3-56c4a47c9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unique_interests[:6] == [\n",
    "    'Big Data',\n",
    "    'C++',\n",
    "    'Cassandra',\n",
    "    'HBase',\n",
    "    'Hadoop',\n",
    "    'Haskell',\n",
    "    # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4c409f-0620-4b19-aeb9-ebdeb593fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a list ofinterests, produce a vector whose ith element is 1\n",
    "    if unique_interests[i] is in the list, 0 otherwise\n",
    "    \"\"\"\n",
    "    return [1 if interest in user_interests else 0\n",
    "            for interest in unique_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d274c35-f974-4d8d-a1bf-4d5e04f56c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
    "                         for user_interests in users_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e89251-09e7-4ff6-82c9-45f452f52be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import import_ipynb\n",
    "import random\n",
    "from nlp import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c5c6f22-f6a1-4066-8d5c-3984eea8a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
    "                      for interest_vector_j in user_interest_vectors]\n",
    "                     for interest_vector_i in user_interest_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7851371-a398-4080-a527-b38e2e332302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.3380617018914066,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1543033499620919,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1889822365046136,\n",
       " 0.5669467095138409,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1690308509457033,\n",
       " 0.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed1dc91a-7063-4ad1-bd48-340f89c2e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users 0 and 9 share interests in Hadoop, Java, and Big Data\n",
    "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eee9872-21a8-4d78-be9b-fc87fdd8ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users 0 and 8 share only one interest: Big Data\n",
    "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90120675-166a-4a6c-96f0-d06407d99ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
    "    pairs = [(other_user_id, similarity)                      # Find other\n",
    "             for other_user_id, similarity in                 # users with\n",
    "                enumerate(user_similarities[user_id])         # nonzero\n",
    "             if user_id != other_user_id and similarity > 0]  # similarity.\n",
    "\n",
    "    return sorted(pairs,                                      # Sort them\n",
    "                  key=lambda pair: pair[-1],                  # most similar\n",
    "                  reverse=True)                               # first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee0c24e0-5b24-4802-8f61-a46faaddf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_to_zero = most_similar_users_to(0)\n",
    "user, score = most_similar_to_zero[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9cdc93-a071-4114-a6ea-56ed89fdfaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0.5669467095138409)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9242d6b-b749-4b3e-b197-bb7d997ba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert user == 9\n",
    "assert 0.56 < score < 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2667866a-2813-45c4-a49f-0101810158cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.3380617018914066)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user, score = most_similar_to_zero[1]\n",
    "user, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00ee1301-6998-4cd2-9481-a6d3995e9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert user == 1\n",
    "assert 0.33 < score < 0.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd22cf2-87a6-4793-957d-2459eaa60de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f15c23-5f69-42e8-a6ec-829b1a628d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Sum up the similarities.\n",
    "    suggestions: Dict[str, float] = defaultdict(float)\n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity\n",
    "\n",
    "    # Convert them to a sorted list.\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[-1],  # weight\n",
    "                         reverse=True)\n",
    "\n",
    "    # And (maybe) exclude already-interests\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e3d30fd-aeee-4699-b883-e95ac95bd547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MapReduce', 0.5669467095138409)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubs0 = user_based_suggestions(0)\n",
    "interest, score = ubs0[0]\n",
    "interest, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5a5d2f4-40a7-4b33-ba59-5f6c3bcd6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert interest == 'MapReduce'\n",
    "assert 0.56 < score < 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "191000c9-910c-4a4e-8757-c9c1252a6a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MongoDB', 0.50709255283711)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest, score = ubs0[1]\n",
    "interest, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3f59465-2bc0-46bb-97b5-0dc34d43b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert interest == 'MongoDB'\n",
    "assert 0.50 < score < 0.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cacb4041-5737-41b2-86dc-9ec9b498a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_user_matrix = [[user_interest_vector[j]\n",
    "                         for user_interest_vector in user_interest_vectors]\n",
    "                        for j, _ in enumerate(unique_interests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cefda21-e056-4eef-9fae-80dc1fa6ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3794cadb-ba21-485a-b419-4cfd2226f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
    "                          for user_vector_j in interest_user_matrix]\n",
    "                         for user_vector_i in interest_user_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "221479e9-caca-449b-b8de-e55321d6a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_interests_to(interest_id: int):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs,\n",
    "                  key=lambda pair: pair[-1],\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93426427-c23d-41e9-b39d-a6a3bdad76f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 0.8164965809277261),\n",
       " ('Java', 0.6666666666666666),\n",
       " ('MapReduce', 0.5773502691896258),\n",
       " ('Spark', 0.5773502691896258),\n",
       " ('Storm', 0.5773502691896258),\n",
       " ('Cassandra', 0.4082482904638631),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('HBase', 0.3333333333333333)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msit0 = most_similar_interests_to(0)\n",
    "msit0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e32f3b0d-9708-431c-9818-997c4afe566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert msit0[0][0] == 'Hadoop'\n",
    "assert 0.815 < msit0[0][1] < 0.817\n",
    "assert msit0[1][0] == 'Java'\n",
    "assert 0.666 < msit0[1][1] < 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a67b0ce8-760c-4efe-801e-c71d31553e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Add up the similar interests\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_vectors[user_id]\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "\n",
    "    # Sort them by weight\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[-1],\n",
    "                         reverse=True)\n",
    "\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28c0d70d-1e2e-4eed-b072-94724566c4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 1.861807319565799),\n",
       " ('Postgres', 1.3164965809277263),\n",
       " ('MongoDB', 1.3164965809277263),\n",
       " ('NoSQL', 1.2844570503761732),\n",
       " ('programming languages', 0.5773502691896258),\n",
       " ('MySQL', 0.5773502691896258),\n",
       " ('Haskell', 0.5773502691896258),\n",
       " ('databases', 0.5773502691896258),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('C++', 0.4082482904638631),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('Python', 0.2886751345948129),\n",
       " ('R', 0.2886751345948129)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[('MapReduce', 1.861807319565799),\n",
    " ('Postgres', 1.3164965809277263),\n",
    " ('MongoDB', 1.3164965809277263),\n",
    " ('NoSQL', 1.2844570503761732),\n",
    " ('programming languages', 0.5773502691896258),\n",
    " ('MySQL', 0.5773502691896258),\n",
    " ('Haskell', 0.5773502691896258),\n",
    " ('databases', 0.5773502691896258),\n",
    " ('neural networks', 0.4082482904638631),\n",
    " ('deep learning', 0.4082482904638631),\n",
    " ('C++', 0.4082482904638631),\n",
    " ('artificial intelligence', 0.4082482904638631),\n",
    " ('Python', 0.2886751345948129),\n",
    " ('R', 0.2886751345948129)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0fb751f-c07f-4520-b390-3e9a6cc1edac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 1.861807319565799),\n",
       " ('MongoDB', 1.3164965809277263),\n",
       " ('Postgres', 1.3164965809277263),\n",
       " ('NoSQL', 1.2844570503761732),\n",
       " ('MySQL', 0.5773502691896258),\n",
       " ('databases', 0.5773502691896258),\n",
       " ('Haskell', 0.5773502691896258),\n",
       " ('programming languages', 0.5773502691896258),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('C++', 0.4082482904638631),\n",
       " ('Python', 0.2886751345948129),\n",
       " ('R', 0.2886751345948129)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibs0 = item_based_suggestions(0)\n",
    "ibs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce7b25d0-0710-4764-ad46-ed0e85179c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ibs0[0][0] == 'MapReduce'\n",
    "assert 1.86 < ibs0[0][1] < 1.87\n",
    "assert ibs0[1][0] in ('Postgres', 'MongoDB')  # A tie\n",
    "assert 1.31 < ibs0[1][1] < 1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee84f803-1718-4003-b633-65f89e83f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    \n",
    "#     # Replace this with the locations of your files\n",
    "    \n",
    "#     # This points to the current directory, modify if your files are elsewhere.\n",
    "#     MOVIES = \"u.item\"   # pipe-delimited: movie_id|title|...\n",
    "#     RATINGS = \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp\n",
    "    \n",
    "#     from typing import NamedTuple\n",
    "    \n",
    "#     class Rating(NamedTuple):\n",
    "#         user_id: str\n",
    "#         movie_id: str\n",
    "#         rating: float\n",
    "    \n",
    "#     import csv\n",
    "#     # We specify this encoding to avoid a UnicodeDecodeError.\n",
    "#     # see: https://stackoverflow.com/a/53136168/1076346\n",
    "#     with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
    "#         reader = csv.reader(f, delimiter=\"|\")\n",
    "#         movies = {movie_id: title for movie_id, title, *_ in reader}\n",
    "    \n",
    "#     # Create a list of [Rating]\n",
    "#     with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
    "#         reader = csv.reader(f, delimiter=\"\\t\")\n",
    "#         ratings = [Rating(user_id, movie_id, float(rating))\n",
    "#                    for user_id, movie_id, rating, _ in reader]\n",
    "    \n",
    "#     # 1682 movies rated by 943 users\n",
    "#     assert len(movies) == 1682\n",
    "#     assert len(list({rating.user_id for rating in ratings})) == 943\n",
    "    \n",
    "#     import re\n",
    "    \n",
    "#     # Data structure for accumulating ratings by movie_id\n",
    "#     star_wars_ratings = {movie_id: []\n",
    "#                          for movie_id, title in movies.items()\n",
    "#                          if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
    "    \n",
    "#     # Iterate over ratings, accumulating the Star Wars ones\n",
    "#     for rating in ratings:\n",
    "#         if rating.movie_id in star_wars_ratings:\n",
    "#             star_wars_ratings[rating.movie_id].append(rating.rating)\n",
    "    \n",
    "#     # Compute the average rating for each movie\n",
    "#     avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
    "#                    for movie_id, title_ratings in star_wars_ratings.items()]\n",
    "    \n",
    "#     # And then print them in order\n",
    "#     for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
    "#         print(f\"{avg_rating:.2f} {movies[movie_id]}\")\n",
    "    \n",
    "#     import random\n",
    "#     random.seed(0)\n",
    "#     random.shuffle(ratings)\n",
    "    \n",
    "#     split1 = int(len(ratings) * 0.7)\n",
    "#     split2 = int(len(ratings) * 0.85)\n",
    "    \n",
    "#     train = ratings[:split1]              # 70% of the data\n",
    "#     validation = ratings[split1:split2]   # 15% of the data\n",
    "#     test = ratings[split2:]               # 15% of the data\n",
    "    \n",
    "#     avg_rating = sum(rating.rating for rating in train) / len(train)\n",
    "#     baseline_error = sum((rating.rating - avg_rating) ** 2\n",
    "#                          for rating in test) / len(test)\n",
    "    \n",
    "#     # This is what we hope to do better than\n",
    "#     assert 1.26 < baseline_error < 1.27\n",
    "    \n",
    "    \n",
    "#     # Embedding vectors for matrix factorization model\n",
    "    \n",
    "#     from scratch.deep_learning import random_tensor\n",
    "    \n",
    "#     EMBEDDING_DIM = 2\n",
    "    \n",
    "#     # Find unique ids\n",
    "#     user_ids = {rating.user_id for rating in ratings}\n",
    "#     movie_ids = {rating.movie_id for rating in ratings}\n",
    "    \n",
    "#     # Then create a random vector per id\n",
    "#     user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
    "#                     for user_id in user_ids}\n",
    "#     movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
    "#                      for movie_id in movie_ids}\n",
    "    \n",
    "    \n",
    "#     # Training loop for matrix factorization model\n",
    "    \n",
    "#     from typing import List\n",
    "#     import tqdm\n",
    "#     from scratch.linear_algebra import dot\n",
    "    \n",
    "#     def loop(dataset: List[Rating],\n",
    "#              learning_rate: float = None) -> None:\n",
    "#         with tqdm.tqdm(dataset) as t:\n",
    "#             loss = 0.0\n",
    "#             for i, rating in enumerate(t):\n",
    "#                 movie_vector = movie_vectors[rating.movie_id]\n",
    "#                 user_vector = user_vectors[rating.user_id]\n",
    "#                 predicted = dot(user_vector, movie_vector)\n",
    "#                 error = predicted - rating.rating\n",
    "#                 loss += error ** 2\n",
    "    \n",
    "#                 if learning_rate is not None:\n",
    "#                     #     predicted = m_0 * u_0 + ... + m_k * u_k\n",
    "#                     # So each u_j enters output with coefficent m_j\n",
    "#                     # and each m_j enters output with coefficient u_j\n",
    "#                     user_gradient = [error * m_j for m_j in movie_vector]\n",
    "#                     movie_gradient = [error * u_j for u_j in user_vector]\n",
    "    \n",
    "#                     # Take gradient steps\n",
    "#                     for j in range(EMBEDDING_DIM):\n",
    "#                         user_vector[j] -= learning_rate * user_gradient[j]\n",
    "#                         movie_vector[j] -= learning_rate * movie_gradient[j]\n",
    "    \n",
    "#                 t.set_description(f\"avg loss: {loss / (i + 1)}\")\n",
    "    \n",
    "#     learning_rate = 0.05\n",
    "#     for epoch in range(20):\n",
    "#         learning_rate *= 0.9\n",
    "#         print(epoch, learning_rate)\n",
    "#         loop(train, learning_rate=learning_rate)\n",
    "#         loop(validation)\n",
    "#     loop(test)\n",
    "    \n",
    "    \n",
    "#     from scratch.working_with_data import pca, transform\n",
    "    \n",
    "#     original_vectors = [vector for vector in movie_vectors.values()]\n",
    "#     components = pca(original_vectors, 2)\n",
    "    \n",
    "#     ratings_by_movie = defaultdict(list)\n",
    "#     for rating in ratings:\n",
    "#         ratings_by_movie[rating.movie_id].append(rating.rating)\n",
    "    \n",
    "#     vectors = [\n",
    "#         (movie_id,\n",
    "#          sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n",
    "#          movies[movie_id],\n",
    "#          vector)\n",
    "#         for movie_id, vector in zip(movie_vectors.keys(),\n",
    "#                                     transform(original_vectors, components))\n",
    "#     ]\n",
    "    \n",
    "#     # Print top 25 and bottom 25 by first principal component\n",
    "#     print(sorted(vectors, key=lambda v: v[-1][0])[:25])\n",
    "#     print(sorted(vectors, key=lambda v: v[-1][0])[-25:])\n",
    "    \n",
    "# if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d7f53-c4b0-4e6e-a061-6eaf2fd794e9",
   "metadata": {},
   "source": [
    "# the main function above uses deep learning module not developed by author besides using movie files without providing the source of the files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
